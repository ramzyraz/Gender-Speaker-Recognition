# -*- coding: utf-8 -*-
"""21100170_21100010.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bjjUYK9BAGVRay4P0Do2rcoM1yphW7GL
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Colab Notebooks/Machine Learning/Project/

"""# Machine Learning Project
Will be used to make the Speaker Recognition Part
"""

!pip install python_speech_features

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import python_speech_features as mfcc
from scipy.io.wavfile import read
import pickle
import glob
import time
import os
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

"""### Shared Functions"""

def get_MFCC(audio, sr):
    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = True)
    return np.mean(features, axis=0)

def get_numpy(elem):
  return np.array(elem)

def generate_weight_matrix(classes,features):
    return np.zeros((classes,features),dtype=np.float32)

def softmax(x):
    e_x = np.exp(x)
    return e_x / e_x.sum(axis = 1,keepdims=True)

def cost_function(X, weights, y):
  epsilon = 10**-10
  preds = predictions(X, weights)
  h = softmax(preds)
  m = y.size
  J = (1/m) * -np.sum(h*np.log(y + epsilon))
  return J

def predictions(X, weights):
  return X @ weights.T

#Gradient Descent function
def gradient_descent(X,y, X2, y2, thetas,alpha,epochs):
    J = []
    J2 = []
    m = y.size
    for i in range(epochs):
        preds = predictions(X, thetas)
        h = softmax(preds)

        delta = 1/m * (X.T@(h-y))
        thetas =(thetas.T - (alpha*delta)).T 
        J.append(cost_function(X, thetas, y))
        J2.append(cost_function(X2, thetas, y2))
        
    return thetas, J, J2

def index_preds(X, weights):
  preds = predictions(X, weights)
  h = softmax(preds)
  return np.argmax(h, axis=1)

def final_accuracies(y_test, y_pred):
  acc_score = (metrics.accuracy_score(y_test, y_pred))
  acc_score = (round(acc_score,2)) * 100

  macro_precision = (metrics.precision_score(y_test, y_pred, average='macro'))
  macro_precision = (round(macro_precision,2)) * 100

  macro_recall = (metrics.recall_score(y_test, y_pred, average='macro'))
  macro_recall = (round(macro_recall,2)) * 100

  macro_f1 = (metrics.f1_score(y_test, y_pred, average='macro'))
  macro_f1 = (round(macro_f1,2))*100

  confusion = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))
  creport = classification_report(y_test, y_pred)

  return acc_score, macro_precision, macro_recall, macro_f1, confusion, creport

"""### Gender Recognition"""

def get_features2(directory):
  features_list = []
  labels_list = []
  for file in glob.iglob(directory):
    if 'F' in file:
      labels_list.append(0)
      sr, audio = read(file)
      features = get_MFCC(audio, sr)
      features_list.append(features)
    elif 'M' in file:
      labels_list.append(1)
      sr, audio = read(file)
      features = get_MFCC(audio, sr)
      features_list.append(features)
      
  return features_list, labels_list

def one_hot_converter(labels,y):
    one_hot = []
    for i in range(len(y)):
        vec = np.zeros((len(labels,)))
        vec[y[i][0]] = 1
        one_hot.append(vec)
    return np.array(one_hot)

starttime = time.time() 
X_train, y_train = get_features2("Dataset/Gender_Recognition/Train/**/*.wav")
print ("Training Reading Time is = ", str(time.time() - starttime) + " seconds")

starttime = time.time() 
X_valid, y_valid = get_features2("Dataset/Gender_Recognition/Valid/**/*.wav")
print ("Validation Reading Time is = ", str(time.time() - starttime) + " seconds")

starttime = time.time() 
X_test, y_test = get_features2("Dataset/Gender_Recognition/Test/**/*.wav")
print ("TestingReading Time is = ", str(time.time() - starttime) + " seconds")

X_train = get_numpy(X_train)
y_train = get_numpy(y_train)
X_valid = get_numpy(X_valid)
y_valid = get_numpy(y_valid)
X_test = get_numpy(X_test)
y_test = get_numpy(y_test)

X_train = np.concatenate([np.ones((len(X_train), 1)), X_train], axis=1)
X_valid = np.concatenate([np.ones((len(X_valid), 1)), X_valid], axis=1)
X_test = np.concatenate([np.ones((len(X_test), 1)), X_test], axis=1)

print("Training Data X", X_train.shape)
print("Validation Data X", X_valid.shape)
print("Testing Data X", X_test.shape)
print()
print("Training Data Y", y_train.shape)
print("Validation Data Y", y_valid.shape)
print("Testing Data Y", y_test.shape)

y_train = y_train.reshape(len(y_train), 1)
y_valid = y_valid.reshape(len(y_valid), 1)
y_test = y_test.reshape(len(y_test), 1)

print("Training Data Y", y_train.shape)
print("Validation Data Y", y_valid.shape)
print("Testing Data Y", y_test.shape)

unique_labels = np.unique(y_train)
unique_labels

len(unique_labels)

one_h_train = one_hot_converter(unique_labels,y_train)
one_h_test  = one_hot_converter(unique_labels,y_test)
one_h_valid = one_hot_converter(unique_labels,y_valid)

print("Hot Encoding Training Y", one_h_train.shape)
print("Hot Encoding Validation Y", one_h_valid.shape)
print("Hot Encoding Testing Y", one_h_test.shape)

weights = generate_weight_matrix(len(unique_labels),X_train.shape[1])
weights.shape

## Epoch and Alphas List:
alphas = [0.001, 0.01, 0.1, 0.5, 1.0]
epochs = [500, 1000, 1500]

"""*** Epoch # 1 ***"""

temp_vcosts_e1 = []
temp_tcosts_e1 = []
for i in range(len(alphas)):
  new_weights, J, J2 = gradient_descent(X_train, one_h_train, X_valid, one_h_valid, weights, alphas[i], epochs[0])
  temp_tcosts_e1.append(J)
  temp_vcosts_e1.append(J2)

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[0])), temp_tcosts_e1[0], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[0])), temp_vcosts_e1[0], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[0], epochs[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[1])), temp_tcosts_e1[1], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[1])), temp_vcosts_e1[1], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[1], epochs[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[2])), temp_tcosts_e1[2])
plt.plot(np.arange(len(temp_vcosts_e1[2])), temp_vcosts_e1[2])
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[2], epochs[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[3])), temp_tcosts_e1[3])
plt.plot(np.arange(len(temp_vcosts_e1[3])), temp_vcosts_e1[3])
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[3], epochs[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[4])), temp_tcosts_e1[4])
plt.plot(np.arange(len(temp_vcosts_e1[4])), temp_vcosts_e1[4])
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[4], epochs[0]))
plt.legend(['Training Data', 'Validation Data'])

"""*** Epoch # 2 ***"""

temp_vcosts_e1 = []
temp_tcosts_e1 = []
for i in range(len(alphas)):
  new_weights, J, J2 = gradient_descent(X_train, one_h_train, X_valid, one_h_valid, weights, alphas[i], epochs[1])
  temp_tcosts_e1.append(J)
  temp_vcosts_e1.append(J2)

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[0])), temp_tcosts_e1[0])
plt.plot(np.arange(len(temp_vcosts_e1[0])), temp_vcosts_e1[0])
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[0], epochs[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[1])), temp_tcosts_e1[1], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[1])), temp_vcosts_e1[1], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[1], epochs[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[2])), temp_tcosts_e1[2], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[2])), temp_vcosts_e1[2], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[2], epochs[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[3])), temp_tcosts_e1[3], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[3])), temp_vcosts_e1[3], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[3], epochs[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[4])), temp_tcosts_e1[4], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[4])), temp_vcosts_e1[4], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[4], epochs[1]))
plt.legend(['Training Data', 'Validation Data'])

"""*** Epoch #3 *** """

temp_vcosts_e1 = []
temp_tcosts_e1 = []
for i in range(len(alphas)):
  new_weights, J, J2 = gradient_descent(X_train, one_h_train, X_valid, one_h_valid, weights, alphas[i], epochs[2])
  temp_tcosts_e1.append(J)
  temp_vcosts_e1.append(J2)

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[0])), temp_tcosts_e1[0], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[0])), temp_vcosts_e1[0], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[0], epochs[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[1])), temp_tcosts_e1[1], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[1])), temp_vcosts_e1[1], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[1], epochs[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[2])), temp_tcosts_e1[2], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[2])), temp_vcosts_e1[2], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[2], epochs[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[3])), temp_tcosts_e1[3], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[3])), temp_vcosts_e1[3], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[3], epochs[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_e1[4])), temp_tcosts_e1[4], lw=2)
plt.plot(np.arange(len(temp_vcosts_e1[4])), temp_vcosts_e1[4], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas[4], epochs[2]))
plt.legend(['Training Data', 'Validation Data'])

"""*** Calculating new Weights using Best Alpha and Epoch ***"""

new_weights, J, J2 = gradient_descent(X_train, one_h_train, X_valid, one_h_valid, weights, 0.1, 500)

new_weights.shape

"""*** Calculating Accuracy ***"""

y_pred = index_preds(X_test, new_weights)

y_pred.shape

acc_score, macro_precision, macro_recall, macro_f1, confusion, classifcation = final_accuracies(y_test, y_pred)

print("Accuracy Score is: {}%".format(acc_score))
print("Macroaveraged Recall is: {}%".format(macro_recall))
print("Macroaveraged Precision is: {}%".format(macro_precision))
print("Macroaveraged F1-score is: {}%".format(macro_f1))

print("\nConfusion Matrix :\n")
print(confusion)

print("\nClassification Report:\n")
print(classifcation)



"""### Speaker Recognition"""

def extract_label(file):
  r = file.split('SPK')[-1]
  return r[:3]

def get_features(directory):
  features_list = []
  labels_list = []
  for file in glob.iglob(directory):
    extracted_label = extract_label(file)
    labels_list.append(extracted_label)
    sr, audio = read(file)
    features = get_MFCC(audio, sr)
    features_list.append(features)
  return get_array(features_list, labels_list)

def get_array(features, labels):
  m = len(features)
  X = np.concatenate([np.ones((m, 1)), features], axis=1)
  X, Y = np.array(X), np.array(list(map(int, labels)))
  return X, Y.reshape(Y.shape[0], 1)

def one_hot_converter2(labels,y):
    one_hot = []
    for i in range(len(y)):
        vec = np.zeros((len(labels,)))
        vec[y[i][0]] = 1
        one_hot.append(vec)
    return np.array(one_hot)

starttime = time.time() 
X_train_speaker, y_train_speaker = get_features("Dataset/Speaker_Recognition/Train/**/*.wav")
print ("Training Reading Time is = ", str(time.time() - starttime) + " seconds")

starttime = time.time() 
X_valid_speaker, y_valid_speaker = get_features("Dataset/Speaker_Recognition/Valid/**/*.wav")
print ("Validation Reading Time is = ", str(time.time() - starttime) + " seconds")

starttime = time.time() 
X_test_speaker, y_test_speaker = get_features("Dataset/Speaker_Recognition/Test/**/*.wav")
print ("TestingReading Time is = ", str(time.time() - starttime) + " seconds")

print("Training Data X", X_train_speaker.shape)
print("Validation Data X", X_valid_speaker.shape)
print("Testing Data X", X_test_speaker.shape)
print()
print("Training Data Y", y_train_speaker.shape)
print("Validation Data Y", y_valid_speaker.shape)
print("Testing Data Y", y_test_speaker.shape)

unique_labels_speaker = np.unique(y_train_speaker)
unique_labels_speaker.shape

one_h_train_speaker = one_hot_converter2(np.unique(y_train_speaker).astype(int)-1, y_train_speaker.astype(int)-1)
one_h_valid_speaker = one_hot_converter2(np.unique(y_valid_speaker).astype(int)-1, y_valid_speaker.astype(int)-1)
one_h_test_speaker  = one_hot_converter2(np.unique(y_test_speaker).astype(int)-1, y_test_speaker.astype(int)-1)

print("Hot Encoding Training Y", one_h_train_speaker.shape)
print("Hot Encoding Validation Y", one_h_valid_speaker.shape)
print("Hot Encoding Testing Y", one_h_test_speaker.shape)

weights_speaker = generate_weight_matrix(one_h_train_speaker.shape[1], X_train_speaker.shape[1])
weights_speaker.shape

## Epoch and Alphas List:
alphas_sp = [0.001, 0.01, 0.1, 0.5, 1.0]
epochs_sp = [2000, 2500, 3000]

"""*** Epoch # 1 ***"""

temp_vcosts_s1 = []
temp_tcosts_s1 = []
for i in range(len(alphas)):
  new_weights, J, J2 = gradient_descent(X_train_speaker, one_h_train_speaker, X_valid_speaker, one_h_valid_speaker, weights_speaker, alphas_sp[i], epochs_sp[0])
  temp_tcosts_s1.append(J)
  temp_vcosts_s1.append(J2)

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[0])), temp_tcosts_s1[0], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[0])), temp_vcosts_s1[0], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[0], epochs_sp[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[1])), temp_tcosts_s1[1], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[1])), temp_vcosts_s1[1], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[1], epochs_sp[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[2])), temp_tcosts_s1[2], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[2])), temp_vcosts_s1[2], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[2], epochs_sp[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[3])), temp_tcosts_s1[3], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[3])), temp_vcosts_s1[3], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[3], epochs_sp[0]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[4])), temp_tcosts_s1[4], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[4])), temp_vcosts_s1[4], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[4], epochs_sp[0]))
plt.legend(['Training Data', 'Validation Data'])

"""*** Epoch # 2 ***"""

temp_vcosts_s1 = []
temp_tcosts_s1 = []
for i in range(len(alphas)):
  new_weights, J, J2 = gradient_descent(X_train_speaker, one_h_train_speaker, X_valid_speaker, one_h_valid_speaker, weights_speaker, alphas_sp[i], epochs_sp[1])
  temp_tcosts_s1.append(J)
  temp_vcosts_s1.append(J2)

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[0])), temp_tcosts_s1[0], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[0])), temp_vcosts_s1[0], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[0], epochs_sp[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[1])), temp_tcosts_s1[1], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[1])), temp_vcosts_s1[1], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[1], epochs_sp[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[2])), temp_tcosts_s1[2], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[2])), temp_vcosts_s1[2], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[2], epochs_sp[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[3])), temp_tcosts_s1[3], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[3])), temp_vcosts_s1[3], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[3], epochs_sp[1]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[4])), temp_tcosts_s1[4], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[4])), temp_vcosts_s1[4], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[4], epochs_sp[1]))
plt.legend(['Training Data', 'Validation Data'])

"""*** Epoch #3 *** """

temp_vcosts_s1 = []
temp_tcosts_s1 = []
for i in range(len(alphas)):
  new_weights, J, J2 = gradient_descent(X_train_speaker, one_h_train_speaker, X_valid_speaker, one_h_valid_speaker, weights_speaker, alphas_sp[i], epochs_sp[2])
  temp_tcosts_s1.append(J)
  temp_vcosts_s1.append(J2)

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[0])), temp_tcosts_s1[0], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[0])), temp_vcosts_s1[0], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[0], epochs_sp[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[1])), temp_tcosts_s1[1], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[1])), temp_vcosts_s1[1], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[1], epochs_sp[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[2])), temp_tcosts_s1[2], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[2])), temp_vcosts_s1[2], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[2], epochs_sp[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[3])), temp_tcosts_s1[3], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[3])), temp_vcosts_s1[3], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[3], epochs_sp[2]))
plt.legend(['Training Data', 'Validation Data'])

# Plotting the Training Cost vs Epoch Graph
fig = plt.figure(figsize=(12,5))
plt.plot(np.arange(len(temp_tcosts_s1[4])), temp_tcosts_s1[4], lw=2)
plt.plot(np.arange(len(temp_vcosts_s1[4])), temp_vcosts_s1[4], lw=2)
plt.xlabel('Number of Epochs')
plt.ylabel('Training and Validation Cost J')
plt.title('Costs vs Iterations for Alpha = {} and Epoch = {}'.format(alphas_sp[4], epochs_sp[2]))
plt.legend(['Training Data', 'Validation Data'])

"""*** Calculating new Weights using Best Alpha and Epoch ***"""

new_weights, J_speaker, valid_speaker = gradient_descent(X_train_speaker, one_h_train_speaker, X_valid_speaker, one_h_valid_speaker, weights_speaker, 1.0, 3000)

new_weights.shape

new_weights

"""*** Calculating Accuracy ***"""

y_pred_update = index_preds(X_test_speaker, new_weights)

y_pred_update.shape

acc_score_speak, macro_precision_speak, macro_recall_speak, macro_f1_speak, confusion_speak, classifcation_speak = final_accuracies(y_test_speaker.astype(int)-1, y_pred_update)

print("Accuracy Score is: {}%".format(acc_score_speak))
print("Macroaveraged Recall is: {}%".format(macro_recall_speak))
print("Macroaveraged Precision is: {}%".format(macro_precision_speak))
print("Macroaveraged F1-score is: {}%".format(macro_f1_speak))

print("\nConfusion Matrix :\n")
print(confusion_speak)

print("\nClassification Report:\n")
print(classifcation_speak)

